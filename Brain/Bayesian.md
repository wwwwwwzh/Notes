

Note: Bayesian belief is more creationist like where everything has a design behind it. This note however, will use a frequentism viewpoint in explaining Bayesian inference in neural network.

# General
In a purely neuroscience point of view, probability starts with understanding with some basic properties and relations of entities. We learn to tell existence of objects and then to count objects with "numbers" (classical example is asking kid to tell how old they are). Then they learn to compare things either explicitly with intuitive orderness of numbers or implicitly (with connection to other properties like area). 

Then with numbers come notion of frequency and likeness. Some episodes happen more often than others and some things are more likely to lead to other things. 

By modeling the brain with Bayesian inference, what we are doing is to replace the ideas and activations of ideas with random variables. They are indeed random variables since they sometimes activate and sometimes don't. Why the math work in cord with the underlying basic properties of brain is another topic, but intuitively the physical process governing brain behaviors are exactly the processes that prompted the invention of probability in the first place. Physical laws are manifested everywhere.

## Frequentist Bayesian 
We see that people wake up more often at 7 than 8 or than 6 and the more it goes away from 7 the less likely. That's our prior. It comes from observation of natural frequency. Now how do we process new observations?

Recently you notice people are waking up more often at 5, say, for a week. What's the probability that this would happen under your original prior? Note that we actually don't update priors very often. We might choose not to believe new information because our prior says it's unlikely. 