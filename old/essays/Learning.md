
This book, for as far as the part I read, fails to achieve its goal of explaining what learning is and give the "good" way to learn. But before explaining this, I need to introduce two concepts that are crucial to understanding any science, especially science about human.

The first is the reverse engineering problem which I call the projection problem, e.g. to recreate the design of a software or jet fighter given the finished software or the fighter. Plato described this as the shadow on the cave wall which is why I like to call it the projection problem (projection of invisible and possibly higher dimensional object to our mostly 2D visual perception). So why is it hard to explain how the brain works, or in general, any physical process works? Suppose a 3D object casts a shadow on a plane, the space of the reverse projections from that shadow, namely the object we are trying to determine, is infinite. We can try casting light from different angles and obtain different shadows, and each time we do, we get a better idea of what that object actually is. However, we can never be certain what it really is AND people like to get quick conclusions from just a few such projections. This is exactly how science has evolved. The castings are experiments and the guess of the object is the hypothesis. Sometimes when enough projections are given we are confident enough to call it a "law". The development of natural science is painful, so should neuroscience/psychology. We can't conduct high precision experiment on human (placing cell precision electrodes in human brain to simulate and record individual neuronal activity). The space of the projection of brain, aka behavior, is terribly vast. People like Freud and B.F. Skinner got too famous whose theories are neuroscience equivalence of geocentric model of Plato, Aristotle and Ptolemy. They are not necessarily wrong, just not good enough to explain more observations. When it comes specifically about learning, people just have no idea of what they are talking about. The psychological experiments and in consequence of these, the current methods of education, are in my view very superficial and ineffective. These methods as to the behavior of learning are like ancient astrology to physics and cosmology. 

The other concept is about language. I like to think of the two most important advancement of civilization as two advancements in language. The first being the creation of human language and the second creation of mathematical language. Language is essentially a projection of mind. We have something in our mind and we want to transmit it. We have to project it to something so it can be reverse engineered by others. Before human language, we are essentially dogs barking at each other. A very crude projection. Extremely low precision but high compression. Animals' language are basically expression of emotions. Combined with body language they can do a lot of things like indicating danger and expressing affection, sufficient for survival and reproduction. Human language is on a different level. Suddenly when you want to warn your fellows of danger you can say exactly what danger it is, how far is it from, etc. It's also an advancement on our internal world, the mind. We hold in mind a representation of the world and manipulate them to remember, explain, imagine, and predict. It used to be just blurry images and sounds and vague concepts. Now with language everything has a name. It produced the level of intelligence we had. As Helen Keller noted when learning her first words: "At first, when my teacher told me about a new thing I asked very few questions. My ideas were vague, and my vocabulary was inadequate; but as my knowledge of things grew, and I learned more and more words, my field of inquiry broadened." Language of art should also be considered as a more abstract language, or a higher level projection that projects extremely high dimensional thoughts to still very high dimensional artworks. After human languages, the invention of mathematical language further made the projection more precise, and some people call it the language of the universe, which I translate to as the ultimate projection mechanism of the physical world. Given that math is more precise than English, it is easy to understand why we misunderstand each other on a wordly basis when using English. Your definition of an Apple will never be the same as mine. When it comes to abstract concepts like do you LIKE apples, it should be understandable that our disagreements sometimes result from a disagreement in definition of words, or a difference in how we internally represent ides and objects with language.

Now let's talk about learning. First of all, my definition of learning here is merely "the change of memory". But memory, is everything. Memory is what defines us, motivates us and what determines our behaviors. I'm writing this because I have this homework in my memory, and I remember that doing homework gives good outcomes. I'm able to write because I remember, implicitly (I have to use some jargons which I'll explain later), how to use the words and grammars and all the other evidence I knew to support my claims. Some memory are born, like the ability to recognize faces (we "remember" what a face should look like) and the desire to eat (we "remember" to eat when hungry). How does memory work then? Memory is a collection of connections, relations, bridges, or one to one functions. Therefore learning is just the process to establish such connections. Think about how when you learned "Washington is the first US president", you connected Washington with "first US president", and more importantly, you connected ideas or internal representations about "Washington", "US", "first", "president" to this whole sentence such that the next time you hear a discussion involving any of these ideas, you will probably be reminded of this sentence. What's the process behind this connection? Very intuitive, 1) the things you are trying to connect and 2) attention (in a very simplified model, attention releases things that strengthens connection between the ideas you are connecting). 

It's also worth explaining the two memory systems in brain, though they could both be explained by the connection model. First is explicit memory. Facts, life episodes, basically anything you are consciously aware of when thinking about it. Another is implicit. This is more nuanced. Things like riding a bike, playing the piano, or even language when you are just "letting your tongue go". They are supported by two distinct physical parts in the brain. What's interesting about these two systems is that one is more probabilistic than the other. Facts and recounting of events are more fixed. Your memory of your first date has an accurate time, place and numerous other details (though sometimes we mess up and these become vague). Riding a bicycle is different. You never learned under what explicit conditions should you do this or that specific manuever, you are reacting to the environment probabilistically. Given all the trials and errors while learning, we have very efficient mechanisms for selecting the highest scoring action.

Now I'll attempt to explain how "thinking" works. It's important to discuss this before doing case studies about arguments in the book because any learning or action or anything we do, could be broadly defined under the framework of "thinking". (Note that I wouldn't attempt to explain the general question of consciousness or the "feeling of being".) Thinking is very similar to mechanical motion wherein you set up some initial conditions and it starts rolling. For example, you are preparing some powerpoints for today's lecture. Every slide you read connects to everything in your brain in parallel and the most prominent idea emerged (highest scoring function). Say that idea is about a mistake in the slide. Then this idea is sent back and connects to everything again and gives another output, perhaps a way to fix it. This loop runs from birth to death. It's worth noting that even though we are not aware of this, the connection matching mechanism is massively parallel. Given one question we have one response, but subconsciously we have many potential responses with lower probability scores that are omitted by a selection mechanism. Also note that the input, or one side of the connection, are in reality very complex. It involves the immediate perception of the world, like a question you heard or a slide you read, and previous internal thoughts. The process of connecting these inputs to an output is also influenced by various factors like emotions and internal senses (you think differently when you have a stomachache or are in love). 

From here I'll explain with this model some of the books' arguments. First, why many common strategies aren't helpful for learning? Before answering this we have to figure out what people mean by "good learning", or "remembering well" (again showing how human language is imprecise and requires constant redefinition). To remember something we really don't mean to recite it when asked to. We mean something like that mentioned earlier about Washington being first US president. We want to recall that piece of information when relevant information shows up. In other words, we want the thing we remembered to come in handy whenever it should. For example, to say I have remembered Bayes' formula, I mean I could recite it when people ask "what is Bayes' formula". I should also be able to talk about it when friends showed me a building in University of Edinburgh named after him. In a practical sense, I also need to apply it whenever appropriate. What might the cafeteria have today? Let's apply Bayes' rule. What if the sun stops rising tomorrow? Let's see what Bayes tells us. As you can see, learning is to establish connections from the idea you are learning to potentially everything else (I personally believe that everything should in fact be connected to everything but that's too computationally expensive so hopefully robots can achieve that). In other words, learning is not to build a list of connections but to build a high dimensional network of connections, like a high dimensional spider web if you will. What's so good about this network is that when one connection to a certain node is lost, there are many others that still go to it. What's better in terms of learning is that when you have a big and well connected web, anything you learn new will, by the process of thinking mentioned earlier, match things on the web. And this matching is fun, it's the moment after learning all the names of British monarchs and actually seeing their tombs and identifying them, or the Eureka moment of "why haven't I thought of this? (A low probability connection is activated and turns out to be of highest interest)". What's good about being fun is that it intrinsically make you focused (gives you concentration) and activates things you need for strengthening connections (remember the two ingredient of learning?)

And why are we poor judges of learning? It's that probabilistic learning and thinking thing. We finished learning something, and the thought mechanism mapped this state to "I've learned so well". You need to tune the connections there to get a right response. Why rereading is useless? Reading is merely perception level activity and establishes connections only within the sentence even if you focus hard while reading. What happens when you have bunch of information connected within but not to anywhere else? You'll hardly ever be reminded of it, let alone use it! You have to think about other relevant information when reading. This is hard because you are iterating through the lower probability score outputs your thinking mechanism gives. This is effortful and explains why we like to follow gut feelings and be irrational. Logic and reason are high level, language based (and the more logical you are, the more mathematical/precise the language should be) objects onto which we have to project our vague ideas multiple times to get an accurate output. (The establishment of logic, or the building of connections underlying logic, is beyond the scope of this essay, but do think about how we connect causes and effects by observing basic physical phenomena (like things falling off edge of a table) and inducing logical construct like "if, then"). This multiple projection is against our thinking mechanism and potentially energy consuming. But with the right methods you can learn to connect things in your brain that bypass this.

Why retrieval is good? It's not good but OK. When you retrieve, you are giving inputs like "what did I learn today?" or "what's that thing about Shakespeare" and eliciting a whole paragraph as response. It strengthens at least some connections such that when given highly relevant information (like in a test), you are reminded of what you learned. But this is not optimal because your connections should be diverse. You should try to retrieve it indirectly by constantly asking questions about how anything could be related to what you learned. Thus one great learning strategy is to just think really hard whenever you learn something new. For example, you learned the story of Napoleon today. Your immediate response might be something like "what a great man" or "he's a megalomaniac". Ignore your first feelings and try searching deeper in your memory net. He's a conqueror. Who are other conquerors? What are their relationships? Oh Napoleon is a fan of Alexander the great. That's a useful connection right there. From there you can study how Alexander is the role model of most European conquerors. You can also connect to Napoleon and Enlightenment. What are the conversations between Laplace and Napoleon? Or Napoleon and French revolution, the consequence of the wars, his funny portraits, etc. When you have thought thoroughly about all these. The idea of Napoleon now really sticks. It has become part of you and so many things can lead you to think about him and give an example during a conversation about him. This also explains why sometimes we feel like great learners after a good movie or an exciting talk. First we are motivated so connections are built fast. Also when we look for relevant resources after, say, a movie about Van Gogh, we have numerous pieces of information about him that are reactivated when we then read something similar. The movie is like a string of information. But when we read more about events and people in it afterwards we connect them as a web. 

This is getting way longer than I thought so I'll address one more argument. Why trying to solve a problem before given the solution is great? When we are told 2x2=4, it's just a connection between 2x2 and 4. But when we try to solve it, we first think about addition and we explore the concept of loops. We might try adding four ones together and still getting 4. We also attempt to say it's not 3 or 5. In any event, the relationship between 2x2 and 4 is made more diverse, not to mention the benefit of saying "that's what I did in primary school" when learning how multiplications are constructed from additions which is in turn from a successor function in an analysis class. I guess this is also why we use examples when learning. Though I have to say that there are so many good examples relating to any math concept yet teachers are obsessed with scenarios like water coming in and out of swimming pool at the same time. I also propose to give more historical context whenever teaching anything. Why? It's those inputs that gave rise to the concept being taught as output. It builds a natural and intuitive connection from a problem to an answer. For example, we should talk about how those founding fathers of probability spent so much intellectual effort to win their gambles at casinos. We should talk about coins and dices and all these naturally symmetrical objects and introduce the frequentist notion of probability before the more abstract Bayesian belief system (I think you have done a great job in regards to this in class). Related is my previous argument about building multiple connections. When we learn history together with the concept, we connected the concepts with questions, people who asked them, and maybe the historical events around that time. Now we have something connected to a bigger knowledge network, namely, the sense of historical progression that could theoretically encapsulate everything, how do we strengthen it? Reading a book three times is useful. Why? First time is just getting familiar with some concepts and jargons, also some implicit feelings about cause and effect within the subject. Second time you could connect more things. But what also happens is that you are retrieving or activating those knowledge between the reads so that they are implicitly strengthened. When you have everything connected and read it a third time, it's merely a proof read. You are just checking if projections of your ideas match with what's written there. Note that this in-between period is critical. You can't just read three times back to back. You need to let the vague senses of that new subject hang around in your mind and merge with other things there. It's even better when you actively mix them, just like how I described the asking question example of Napoleon. Newton says that he discovered all those wonderful things and achieved all his glories by just "thinking about them continuously". Note how "continuously" means during that period, he is probably trying to mix everything he sees on the streets to the problem he's trying to solve.

Disclaimer: most concepts introduced here are adaptations from established neuropsychological models of human mind. I have however altered some of them to suit readers without neuroscience background and omitted certain aspects of them to boost integrity of the whole argument.  