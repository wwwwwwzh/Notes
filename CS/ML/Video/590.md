# Dataset
## Simple Classification
### UCF 101
13,320-101 
![](/images/UCF-101.png)

### HMDB 51
6,766-51

### Kinetics 400, 600
240,000-400
390,000-600

### IG-65M

### Something Something V2 
220k-174
![](/images/ssv2.png)

## Video Description
### HowTo100M
130m(~7m)-23k

## Instance Segmentation
### Youtube-VOS (Video Object Segmentation)
4453(3-6s)-94
### Youtube-VIS

## Egocentric
### Ego4D
3670h, preprocessed with diverse accompanied data, 3 (past,present,future) classes of benchmarks

# Methods
## Supervised
### LSCN 2015
- Summary: CNN for image frames in image, RNN for variable length temporal aggregation.
- Result: Slight improvement in classification than single frame
![](/images/LRCN.png)

### Conv 3D (C3D) 2015
- Summary: 3D conv on video
- Input: 3x16x128×171.
- Network: 3x3x3 kernel is best, 5 conv, 5 max pool, 2 fc
- Others: 1) C3D captures appearance for the first few frames but thereafter only attends to salient motion. 2) feature embeddings are very separable

### Kinetics I3D 2018
- Summary: dataset; comparison of methods; pretrained two stream inflated 2D conv (copy in temporal dimension and normalize) with inception backbone
- Motivation: bring benefits of pretraining on large dataset and deep network of image models to video model
- Result: 98% on UCF-101, 74% on Kinetics. Pretraining gives ~2% boost. RGB+flow gives ~3% boost than RGB alone.

![](/images/i3d-1.png)
![](/images/i3d-2.png)

### R3D 2018
- Summary: comparison of methods, 2D+1D separable filter 
- Input: 128 × 171 -> randomly cropping windows of 112 × 112. 8 or 16 frames per clip
- Result: comparable with I3D

![](/images/R3D-1.png)
![](/images/R3D-2.png)

### SlowFast 2019
- Summary: temporal difference for 2 pathways, one low frame rate, high channel depth for spatial info, one high frame rate, low channel depth for temporal info

![](/images/slowfast.png)
> C means channel depth, T means temporal depth
> Lateral fusion means reshaping fast pathway output to slow pathway output so they are exact same dimensions.

### TimesFormer 2021
- Summary: First fully self attention attempt on video. ViT backbone. Divided space time attention. 
- Input: 8x224x224->HxWx3xF->PxPxNx3xF->DxNxF->time->space->DxNxF->D->class
- Result: ~80 on K400; ~60 on SSv2

Ablation:
![](/images/timesformer1.png)
Feature space:
![](/images/timesformer2.png)

### [ViViT 2021](https://arxiv.org/pdf/2103.15691.pdf)
- Summary: plain vit for video, tokenize with CNN, several spatial temporal attention methods explored
![](/images/vivit.png)

## Self-Supervised
### [VideoGPT 2021](https://wilson1yan.github.io/videogpt/index.html)
- Summary: a minimal adaptation of VQ-VAE and GPT architectures for videos.
- Arch: ![](/images/videogpt.png)

### [Video-MAE 2022](https://arxiv.org/pdf/2203.12602.pdf)
![](/images/video-mae-n.png)
![](/images/video-mae.png)

### [Video-MAE v2 2023](https://scholar.google.com/scholar_lookup?arxiv_id=2303.16727)
![](/images/videomae-v2.png)

### [MaskViT 2022](https://maskedvit.github.io/)
- Summary: video prediction based on masked visual modeling (MVM), single frame to video.
- Architecture: ![](/images/maskvit.png)

### [MAGVIT 2023](https://magvit.cs.cmu.edu/)
![](/images/magvit.png)

### [MAGVIT v2 2024](https://magvit.cs.cmu.edu/v2/)

### [OpenAI VPT Minecraft 2022](https://openai.com/research/vpt)


## Segmentation
### Mask-Track RCNN & Youtube-VIS
Summary: simultaneous detection, segmentation and tracking of object instances in videos

