# Statement
The three most pressing problem as of end of 2023 are:
1. On a behavioral level, models should have the concept of truth whence rigorous logic can grow. This might mean the model have to be initiated with predefined "truth" (e.g. seeing is believing, everything is about surviving etc)
2. Still on behavioral level, models should incorporate all other human sensorimotor modalities, most importantly to aid interaction with the world.
3. On implementation level, a new training scheme based on simple algebra and probability should replace current complex and unstable gradient based methods.

# Training

# Association
Whether we use language or images or whatever inputs and outputs, a network connects inputs to outputs, replace those words with "ideas" and we see that a network can be interpreted exactly as an association machine. 

## On Convergence of Inference
### Bias Variance Tradeoff
If network complexity is low, the final result has to be similar?

### Parameter VS Idea (Implicit VS Explicit Thought)
It seems that parameters are just numbers and they can not be interpreted as we interpret inputs or outputs which carry real life meanings. The key thing to remember here is that we don't think with the hidden variables, we think with the explicit things, the input output of network, or our inner speech. The grammar behind our daily language are difficult to query without training that connects abstract words to these common convergence areas. Some are almost impossible to query like pattern of sounds (native speakers don't know how their sounds correspond to written symbols).  We can only indirectly query our mind by sending a thought in and get a thought out. An idea of verbs following subject before learning what verb or subject is can only be done by sending in numerous "verbs" and "subjects" and define whatever the invariant part of the output is as our idea of verb or subject (But this ultimately relates to where consciousness resides since intermediate neurons might also be part of access consciousness) 

An extreme example would be people speaking all kinds of different languages can do essentially the same things. The hidden variables (language), with the ultimate goal being the same (drives of human nature), evolved very differently. Different languages are just like parameters in models trained with different initializations. They are numerically different. But because the training data are the same, ie the world is invariant, there will be a mapping from the different set of hidden variables to the same objects in real world (See Convergence of Evolution)

However, since the world is too complex, our hidden variables don't even necessarily map to common real world objects. e.g. there might be a common idea to be generous for 2 people, but they have different reasons. The ML analogy would be that a certain neuron always activates when the input might lead to generous behavoirs and there is a linear relatioinshiip between activation of this neuron and generousity output by the network. They are connencted to different traininig data (real observatioins) but have the same function. 

Note though that I as a network with different traiiniing distributioin than the 2 described above is makiing this argument, which means I'm connecting these to my hidden neuron that inculudees both. Nonetheless, even without me, the 2 above should understand each other because another related idea common ini their notioins are the same, maybe the actiion to give or say similarly good words. 

Also see Associationism/Thoughts/Explanations/Hidden Neurons

### Convergence of Evolution and Natural Symmetry
We have different languages that serve the same purpose, animals evolve in different trajectories but sometimes evolved the same functions and structures. In ML, we see hidden variables with "meaning". If one such neuron has a linear relationship with what we see as "time" then we say this neuron understands time. Note that time is a human inference. We don't sense or see time, it's a useful guess or explanation just as we use god to explain things. The interesting thing here is that both humans and ANN developed the same notion of time. This together with the evolution example might suggest there is somehow some structures of the universe that, no matter how you interpret it, will always be superior to other explanations. Think about the phylogenetic tree. That's both how evolution works and the best guess we have. This might be related some fundamental properties of symmetry.

### Uncategorized
#### Convergence of Language
- Superioirity of sound as language, alternative possibilities: purely visual (can't output), written (producing sound is more innate than using tools to write), tactile (? can't be passed on, language is generational effort)
- All languages are very similar in structure. 

#### Convergence of Idea
- Politics
- Science: Here we have abundant examples of non convergence. Different cultures developed drastically different guesses about the world. But only one stream of thought persisted becaus it can still explain almost every observations.

## Emergence of Structures From Association
Is it possible that starting with certain connections and largely random weights, networks always find the same input topographical structure (e.g. we all can identify face or snake early on)

# World Model
Observation of the real world helps build a model of how the world should be. In a purely associationistic viewpoint, we can in fact only "observe" the world by connecting inputs in all dimensions. When we have connected observed with other observed, we know how the world should work. e.g. the sun should appear above the horizon, an object should fall down, not up...

## Kid Behavior
A unifying theme of kids' behaviors is connecting one sense with another. They are interested when you both show up and talk or move like something they know. They see things and want to touch it and hold it and see it from different views, sometimes they smell and lick it. They try to shake people because they shook bottles and they saw water turning to an interesting pattern. They threw things off the table to see how it ends. They move and separate things so they know objects are fundamental units and can be arranged. They manipulate the objects to observe their movements. 

But an important aspect of this connection is that they can all be connected to a common central abstract system mediated by language. With this they can connect sound of elephant to look of giraffe to taste banana, and they ask why they can't do that. This adversarial connection is symbolic largely language only.

## Vision
lanague itself is very powerful, alwaredy structured, visual is complex, maybe visual+actoin joint 
attention 

### Classification
It's remarkable that real neurons have patterns just like learned neurons. This might be that both evolution and AI chose to start from classification as the starter task and it's interesting that this task suits the most basic biological and commercial needs. 

### Evolutional Needs for Vision
- Classification: day or night, danger or not
- Identification (More Classification)
- Navigation (Follow)
- Obstruction completion
- Prediction: what's further ahead or after a rock or under the sand
- Navigation (Grabbing)
- Movement of hand and motor correspondence
- Viewpoint change understanding
- 3D completion
- Segmentation

### Motion
#### Static Evolution and Tracking
We pay attention to things that move. But more precisely, things that change appearance or things that move through space. We understand early on the difference, and we mostly care about moving.

## Language
### Evolutional Needs for Language
- Classification: finding relative, mating, emotion, simple events
- 
## Language & Vision
### Evolutional Comparison
Language starts with classification of sound while vision starts with that of light.

Vision in AI is harder because the inputs are "raw". Language is well structured. The words are already clustered into easily separable domains and by formalizing random sounds or ideas which are infinite just like all possible visual stimuli into well defined symbols, the dimensionality reduced to a very finite one with words already scattering nicely alone each axis. 

After the classification task usage for language and vision diverged. We identify items and dangers, plan actions to see obstructed objects or grab things, construct mental maps to navigate, abstract actions to mimic, etc. For language, we also start with identifying items with words, then relationship of words as an abstraction of relationship of objects, (language dependece on visiion?)

We thus need more processing for visual inputs in AI than language. In language, we use CLIP embedding which finds representations that likely align with the real brain representations. In vision we need to first 