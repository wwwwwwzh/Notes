# Statement
The three most pressing problem as of end of 2023 are:
1. On a behavioral level, models should have the concept of truth whence rigorous logic can grow. This might mean the model have to be initiated with predefined "truth" (e.g. seeing is believing, everything is about surviving etc)
2. Still on behavioral level, models should incorporate all other human sensorimotor modalities, most importantly to aid interaction with the world.
3. On implementation level, a new training scheme based on simple algebra and probability should replace current complex and unstable gradient based methods.

# Training

# Association
Whether we use language or images or whatever inputs and outputs, a network connects inputs to outputs, replace those words with "ideas" and we see that a network can be interpreted exactly as an association machine. 

## On Convergence of Inference
### Bias Variance Tradeoff
If network complexity is low, the final result has to be similar?

### Parameter VS Idea (Implicit VS Explicit Thought)
It seems that parameters are just numbers and they can not be interpreted as we interpret inputs or outputs which carry real life meanings. The key thing to remember here is that we don't think with the hidden variables, we think with the explicit things, the input output of network, or our inner speech. The grammar behind our daily language could never be explicitly queried? We can only indirectly query our mind by sending a thought in and get a thought out. An idea of verbs following subject before learning what verb or subject is can only be done by sending in numerous "verbs" and "subjects" and define whatever the invariant part of the output is as our idea of verb or subject (But this ultimately relates to where consciousness resides since intermediate neurons might also be part of access consciousness) 

An extreme example would be people speaking all kinds of different languages can do essentially the same things. The hidden variables (language), with the ultimate goal being the same (drives of human nature), evolved very differently. Different languages are just like parameters in models trained with different initializations. They are numerically different. But because the training data are the same, ie the world is invariant, there will be a mapping from the different set of hidden variables to the same world (why)

Also see Associationism/Thoughts/Explanations/Hidden Neurons
