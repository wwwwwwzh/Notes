# Statement
The three most pressing problem as of end of 2023 are:
1. On a behavioral level, models should have the concept of truth whence rigorous logic can grow. This might mean the model have to be initiated with predefined "truth" (e.g. seeing is believing, everything is about surviving etc)
2. Still on behavioral level, models should incorporate all other human sensorimotor modalities, most importantly to aid interaction with the world.
3. On implementation level, a new training scheme based on simple algebra and probability should replace current complex and unstable gradient based methods.


# Training
## Traditional ML
### Reinforcement
Reinforcement learning is essentially supervised. Extreme case is batch size of one. Batch size of one converges fine with enough data. That is just Bayesian learning. But even without big data, keep in mind that we recall past related experience such that some "typical" or principal trials will have more weight.

## Human 
### Curiosity
### Order Following
We seem to follow orders by default: your friend ask you to hand you a paper and you do it without asking; ancient people do whatever a king asks without questioning or emerging thoughts about "equality" or "freedom". Kids seem to do everything they are told to: at home they do whatever they are asked by parents and at school teachers. A closer examination reveals that there are counter examples and it might not be that we are born to follow orders, but that some evolutionarily deeper mechanisms set motion for order following in some specific situations which happen to be common in childhood. Parents are followed because they are right and only thing to follow at the very start. They provide means for survival so their actions are naturally connected to things that are good and should be followed. Some other mechanisms include fear aversion for doctors, social and other innate emotions for teacher and other relatives.

### Order Rejection

# Association
Whether we use language or images or whatever inputs and outputs, a network connects inputs to outputs, replace those words with "ideas" and we see that a network can be interpreted exactly as an association machine. 

## All Networks Are the Same
### Hopfield and Transformer
https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/

## On Convergence of Inference
### Bias Variance Tradeoff
If network complexity is low, the final result has to be similar?

### Parameter VS Idea (Implicit VS Explicit Thought)
It seems that parameters are just numbers and they can not be interpreted as we interpret inputs or outputs which carry real life meanings. The key thing to remember here is that we don't think with the hidden variables, we think with the explicit things, the input output of network, or our inner speech. The grammar behind our daily language are difficult to query without training that connects abstract words to these common convergence areas. Some are almost impossible to query like pattern of sounds (native speakers don't know how their sounds correspond to written symbols).  We can only indirectly query our mind by sending a thought in and get a thought out. An idea of verbs following subject before learning what verb or subject is can only be done by sending in numerous "verbs" and "subjects" and define whatever the invariant part of the output is as our idea of verb or subject (But this ultimately relates to where consciousness resides since intermediate neurons might also be part of access consciousness) 

An extreme example would be people speaking all kinds of different languages can do essentially the same things. The hidden variables (language), with the ultimate goal being the same (drives of human nature), evolved very differently. Different languages are just like parameters in models trained with different initializations. They are numerically different. But because the training data are the same, ie the world is invariant, there will be a mapping from the different set of hidden variables to the same objects in real world (See Convergence of Evolution)

However, since the world is too complex, our hidden variables don't even necessarily map to common real world objects. e.g. there might be a common idea to be generous for 2 people, but they have different reasons. The ML analogy would be that a certain neuron always activates when the input might lead to generous behavoirs and there is a linear relatioinshiip between activation of this neuron and generousity output by the network. They are connencted to different traininig data (real observatioins) but have the same function. 

Note though that I as a network with different training distribution than the 2 described above is making this argument, which means I'm connecting these to my hidden neuron that includes both. Nonetheless, even without me, the 2 above should understand each other because another related idea common ini their notions are the same, maybe the action to give or say similarly good words. 

Also see Associationism/Thoughts/Explanations/Hidden Neurons

### Convergence of Evolution and Natural Symmetry
We have different languages that serve the same purpose, animals evolve in different trajectories but sometimes evolved the same functions and structures. In ML, we see hidden variables with "meaning". If one such neuron has a linear relationship with what we see as "time" then we say this neuron understands time. Note that time is a human inference. We don't sense or see time, it's a useful guess or explanation just as we use god to explain things. The interesting thing here is that both humans and ANN developed the same notion of time. This together with the evolution example might suggest there is somehow some structures of the universe that, no matter how you interpret it, will always be superior to other explanations. Think about the phylogenetic tree. That's both how evolution works and the best guess we have. This might be related some fundamental properties of symmetry.

### Convergence as Alignment
We align modalities because we have a common reference frame, the physical world. We want machines to also align, and we train them to align based on our own alignments. Thus, humans and NN are also aligned and aligned wrt to the physical world. Such alignment means if we as humans want an image to have certain properties described in words, we can also ask NN, since our words and machine tokens are aligned, to change the image in a way that's aligned to changes observed by both human and machine in data.

### Convergence of Science as a Result of Convergence of Evolution
We have the ability to identity something is falling, but this specific ability might not be attributed to an evolutional event. It's more likely that we acquired the ability to see any motion under evolutional stress (hunting, escaping). This stress itself is a result of nature (organisms move based ultimately on the fundamental forces). With this stress that's from nature we are given a structure again based on nature that can infer nature. This is the most amazing fact about human and universe, a creation that understands the creator.

### Convergence of Human Behaviors 
(See Training/Human/) Throughout childhood for humans and early months of most animals, kids learn or reemember that some predictions/actions/outputs among all seemingly possible ones are better than others. For example in primary school we fill in blanks. Some answers are right but not what teachers want. 

### Uncategorized
#### Convergence of Language
- Superioirity of sound as language, alternative possibilities: purely visual (can't output), written (producing sound is more innate than using tools to write), tactile (? can't be passed on, language is generational effort)
- All languages are very similar in structure. 

#### Convergence of Idea
- Politics
- Science: Here we have abundant examples of non convergence. Different cultures developed drastically different guesses about the world. But only one stream of thought persisted becaus it can still explain almost every observations.

## Emergence of Structures From Association
Is it possible that starting with certain connections and largely random weights, networks always find the same input topographical structure (e.g. we all can identify face or snake early on)

# World Model
Observation of the real world helps build a model of how the world should be. In a purely associationistic viewpoint, we can in fact only "observe" the world by connecting inputs in all dimensions. When we have connected observed with other observed, we know how the world should work. e.g. the sun should appear above the horizon, an object should fall down, not up...

## Kid Behavior
A unifying theme of kids' behaviors is connecting one sense with another. They are interested when you both show up and talk or move like something they know. They see things and want to touch it and hold it and see it from different views, sometimes they smell and lick it. They try to shake people because they shook bottles and they saw water turning to an interesting pattern. They threw things off the table to see how it ends. They move and separate things so they know objects are fundamental units and can be arranged. They manipulate the objects to observe their movements. 

But an important aspect of this connection is that they can all be connected to a common central abstract system mediated by language. With this they can connect sound of elephant to look of giraffe to taste banana, and they ask why they can't do that. This adversarial connection is symbolic largely language only.

### Development of Connections
Kids use only one modality at a time. I explored the world visually and my thoughts were primarily visual imagination. I was more drawn to the visual features as well. Kids open their eyes wide and see the world in detail. This establishes strong in-modality connections. Everything else was a quick response to immediate stimuli and caused a turbulence in memory that doesn't leave permanent traces. 

## Long Short Term Memory
See Focus/Low Level
### LSTM for any Network
Maybe during GD, some big updates mean it's short term, and slow ones mean it's long term. Maybe this is even the case in brain. Maybe the brain is just one big homogeneous network but some parts, because of their nature of being adjacent to inputs, update faster and are thus "short" and those away from primary modalities, eg, limbic system, updates slower.

## Vision
Language itself is very powerful, already structured, visual is complex, maybe visual+action joint 
attention 

### Classification
It's remarkable that real neurons have patterns just like learned neurons. This might be that both evolution and AI chose to start from classification as the starter task and it's interesting that this task suits the most basic biological and commercial needs. 

### Evolutional Needs for Vision
- Classification: day or night, danger or not
- Identification (More Classification)
- Navigation (Follow)
- Obstruction completion
- Prediction: what's further ahead or after a rock or under the sand
- Navigation (Grabbing)
- Movement of hand and motor correspondence
- Viewpoint change understanding
- 3D completion
- Segmentation

### Motion
#### Static Evolution and Tracking
We pay attention to things that move. But more precisely, things that change appearance or things that move through space. We understand early on the difference, and we mostly care about moving.

## Language
### Evolutional Needs for Language
- Classification: finding relative, mating, emotion, simple events
- 
## Language & Vision
### Evolutional Comparison
Language starts with classification of sound while vision starts with that of light.

Vision in AI is harder because the inputs are "raw". Language is well structured. The words are already clustered into easily separable domains and by formalizing random sounds or ideas which are infinite just like all possible visual stimuli into well defined symbols, the dimensionality reduced to a very finite one with words already scattering nicely alone each axis. 

After the classification task usage for language and vision diverged. We identify items and dangers, plan actions to see obstructed objects or grab things, construct mental maps to navigate, abstract actions to mimic, etc. For language, we also start with identifying items with words, then relationship of words as an abstraction of relationship of objects, (language dependence on vision?)

We thus need more processing for visual inputs in AI than language. In language, we use CLIP embedding which finds representations that likely align with the real brain representations. In vision we need to first 

### Convergence of World Model with VL Model
Classification alone is very powerful since we need many deeper structures to do it. But VL is just a more complex classification task with overlapping sets of labels for each image. In humans, when we learn to say thank you, we are classifying situations, or when 